{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face-Recognition-using-FaceNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJnvzJZBq84OdamaigaS0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lill98/Face-Recognition-using-FaceNet/blob/master/Face_Recognition_using_FaceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XCPDnNdz9OE",
        "colab_type": "text"
      },
      "source": [
        "#Flow:\n",
        "1.   Take dataset \n",
        "2.   Align_mtcnn\n",
        "3.   Main\n",
        "4.   Train & save\n",
        "5.   Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0HEYqZO0DNZ",
        "colab_type": "text"
      },
      "source": [
        "##1.   Take dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmsxSEoi0MXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZXMam-6zuwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import,division,print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn_-_cnu0KPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(path):\n",
        "    def get_image_path(file_image_path):\n",
        "        images_paths=[]\n",
        "        images=os.listdir(file_image_path)\n",
        "        num_image=len(images)\n",
        "        #print(\"num_image={}\".format(num_image))\n",
        "        for i in range(num_image):\n",
        "            image_path=os.path.join(file_image_path,images[i])\n",
        "            #print(image_path)\n",
        "            \n",
        "            images_paths.append(image_path)\n",
        "        return images_paths\n",
        "    \n",
        "    class ImageClass():\n",
        "        def __init__(self,name,image_paths):\n",
        "            self.name=name\n",
        "            self.image_paths=image_paths\n",
        "\n",
        "        def __str__(self):\n",
        "            return 'list have 2 attribute: name & image_paths. Detail  ' + 'name_label  '+self.name+'  ,  '+'have   '+str(len(self.image_paths))+'   images' \n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.image_paths)\n",
        "    \n",
        "    def get_datasets(path):\n",
        "        dataset=[]\n",
        "        path_exp=os.path.expanduser(path)\n",
        "        classes_label=os.listdir(path_exp)\n",
        "        print(\"before sort classer_label  \",classes_label)\n",
        "\n",
        "        classes_label.sort()\n",
        "        print(\"after sort classer_label  \",classes_label)\n",
        "        \n",
        "        #logging.debug(classes_label)\n",
        "        number_classes=len(classes_label)\n",
        "        for i in range (number_classes):\n",
        "            file_image_path=os.path.join(path,classes_label[i])\n",
        "            #print(\"file_image_path={}\".format(file_image_path))\n",
        "            #logging.debug(file_image_path)\n",
        "            \n",
        "            if os.path.isdir(file_image_path):\n",
        "                image_paths=get_image_path(file_image_path)\n",
        "                #print(\"image_paths={}\".format(image_paths))\n",
        "                class_name=classes_label[i]\n",
        "                #print(type(ImageClass(class_name,image_paths)))\n",
        "                #print(image_path)\n",
        "                dataset.append(ImageClass(class_name,image_paths))\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    dataset=get_datasets(path)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frq1Zh2R0fsY",
        "colab_type": "text"
      },
      "source": [
        "2.   Align_mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlrp66CC0VLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output(path,name):\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "  new_folder=os.path.join(path,name)\n",
        "  if not os.path.exists(new_folder):\n",
        "    os.makedirs(new_folder)\n",
        "  return new_folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4W0CK9U0ikl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_mtcnn(dataset,output_path):\n",
        "    import cv2\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mtcnn.mtcnn import MTCNN\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "    detector=MTCNN()\n",
        "    #plt.figure(figsize=(10,10))\n",
        "    for i in range(len(dataset)):\n",
        "    #plt.subplot(1,2,i+1)\n",
        "        image_path=dataset[i].image_paths[0]\n",
        "        image_name=dataset[i].name\n",
        "        logging.debug(image_name)\n",
        "        file_image_path=get_output(output_path,image_name)\n",
        "        image=cv2.imread(image_path)\n",
        "        image=cv2.resize(image,(250,250))\n",
        "        result=detector.detect_faces(image)\n",
        "        for person in result:\n",
        "            bounding_box=person[\"box\"]\n",
        "            start_y=person[\"box\"][1]\n",
        "            end_y=person[\"box\"][3]+start_y\n",
        "            start_x=person[\"box\"][0]\n",
        "            end_x=person[\"box\"][2]+start_x\n",
        "            image_crop=image[start_y:end_y,start_x:end_x,:]\n",
        "            _,name_image=os.path.split(image_path)\n",
        "            logging.debug(name_image)\n",
        "            path_image_out=os.path.join(file_image_path,name_image)\n",
        "            if not os.path.exists(path_image_out):\n",
        "              cv2.imwrite(path_image_out,image_crop)\n",
        "            \n",
        "            cv2_imshow(image_crop)\n",
        "            print(image_crop.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mgWyfmX0noG",
        "colab_type": "text"
      },
      "source": [
        "3.   Main\n",
        "\n",
        "lấy emb_array vs labels của ảnh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNs2uRwb0lED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(input_path,dataset,model_path):\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.Session() as sess:\n",
        "            def get_image_paths_and_labels(dataset):\n",
        "                image_paths_flat=[]\n",
        "                labels_flat=[]\n",
        "                for i in range(len(dataset)):\n",
        "                    image_paths_flat+=dataset[i].image_paths\n",
        "                    labels_flat+=[i]*len(dataset[i].image_paths)\n",
        "                return image_paths_flat,labels_flat\n",
        "            dataset=get_dataset(input_path)\n",
        "\n",
        "            paths,labels=get_image_paths_and_labels(dataset)\n",
        "            print(\"paths:   {} \\n labels:   {}\".format(paths,labels))\n",
        "\n",
        "            from tensorflow.python.platform import gfile\n",
        "            def load_model(model, input_map=None):\n",
        "                # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n",
        "                #  or if it is a protobuf file with a frozen graph\n",
        "                model_exp = os.path.expanduser(model)\n",
        "                if (os.path.isfile(model_exp)):\n",
        "                    print('Model filename: %s' % model_exp)\n",
        "                    with gfile.FastGFile(model_exp,'rb') as f:\n",
        "                        graph_def = tf.GraphDef()\n",
        "                        graph_def.ParseFromString(f.read())\n",
        "                        tf.import_graph_def(graph_def, input_map=input_map, name='')\n",
        "                else:\n",
        "                    print('Model directory: %s' % model_exp)\n",
        "                    meta_file, ckpt_file = get_model_filenames(model_exp)\n",
        "                    \n",
        "                    print('Metagraph file: %s' % meta_file)\n",
        "                    print('Checkpoint file: %s' % ckpt_file)\n",
        "                \n",
        "                    saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n",
        "                    saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n",
        "\n",
        "            load_model(model_path)\n",
        "\n",
        "            images_placeholder=tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
        "            embeddings=tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
        "            phase_train_placeholder=tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
        "            embedding_size=embeddings.get_shape()[1]\n",
        "            \n",
        "            def to_rgb(img):\n",
        "                w,h=img.shape\n",
        "                ret=np.empty((w,h,3),dtype=np.uint8)\n",
        "                ret[:,:,0]=ret[:,:,1]=ret[:,:,2]=img\n",
        "                return ret\n",
        "            def prewhiten( x):\n",
        "                mean = np.mean(x)\n",
        "                std = np.std(x)\n",
        "                std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n",
        "                y = np.multiply(np.subtract(x, mean), 1/std_adj)\n",
        "                return y  \n",
        "            from PIL import Image\n",
        "            def load_data(path,image_size):\n",
        "                nrof_samples=len(path)\n",
        "                print(\"len(nrof_sample)=   \",nrof_samples)\n",
        "                images=np.zeros((nrof_samples,image_size,image_size,3))\n",
        "                for i in range(nrof_samples):\n",
        "                    img=cv2.imread(path[i])\n",
        "                    img=cv2.resize(img,(160,160,3))\n",
        "                    #img=np.array(Image.open(path[i]))\n",
        "                    #img=np.resize(img,(160,160,3))\n",
        "                    #print(\"image before standard deviation\",img)\n",
        "                    img=prewhiten(img)\n",
        "                    #print(\"image after  standard deviation\",img)\n",
        "                    #print(\"kich thuoc cua anh {}\".format(img.shape))\n",
        "                    if(img.ndim==2):\n",
        "                        img=to_rgb(img)\n",
        "                    images[i,:,:,:]=img\n",
        "                    #print(\"img =  {}\".format(img))\n",
        "                    #print(\"images[0,:,:,:]=   {}\".format(images[0,:,:,:]))\n",
        "                return images\n",
        "            \n",
        "            batch_size=1\n",
        "            image_size=160\n",
        "            nrof_images=len(paths)\n",
        "            nrof_batches_per_epoch=int(math.ceil(1.0*nrof_images/batch_size))\n",
        "            emb_array=np.zeros((nrof_images,embedding_size))\n",
        "            #print(emb_array)\n",
        "            for i in range(nrof_batches_per_epoch):\n",
        "                start_index=i*batch_size\n",
        "                end_index=min((i+1)*batch_size,nrof_images)\n",
        "                paths_batch=paths[start_index:end_index]\n",
        "                images=load_data(paths_batch,image_size)\n",
        "                \n",
        "                #print(\"images_i=   {}\".format(images))\n",
        "                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
        "                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n",
        "                #print(\"emb_array_i=  {}\".format(emb_array))\n",
        "            #print(\"full_emb_array=  {}\".format(emb_array))\n",
        "            \n",
        "            class_names=[cls.name.replace('_',' ')for cls in dataset]\n",
        "            return (emb_array,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9zdYw9L07X1",
        "colab_type": "text"
      },
      "source": [
        "4.   Train & save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZQlXWeq04a2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_distance(dataset,output_path,emb_train,name_output_directory):\n",
        "    if not os.path.exists(name_output_directory):\n",
        "      os.makedirs(name_output_directory)\n",
        "\n",
        "    class_names=[cls.name.replace('_',' ')for cls in dataset]\n",
        "    with open(output_path,'wb') as outfile:\n",
        "        pickle.dump((emb_train,class_names),outfile)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQN9o3Hp1IyX",
        "colab_type": "text"
      },
      "source": [
        "5.   Recognition: Caculate distance between Data and input image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XHJbCHL1FMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_recognize(emb_array_test,trained_emb_test_path):\n",
        "    # print(emb_array_test.shape)\n",
        "    # print(emb_array_train.shape)\n",
        "\n",
        "    with open(trained_emb_test_path,'rb') as infile:\n",
        "      (emb_array_train,class_names)=pickle.load(infile)\n",
        "    print(\"(class_names=  :{})\".format(class_names))\n",
        "    print(\"leng emb_array=\",len(emb_array_train))\n",
        "    print(\"finished load classifier model\")\n",
        "    # for h in range(len(emb_array_test)):\n",
        "    #print(\"nguoi thu {}\".format(h))\n",
        "\n",
        "    for k in range(len(emb_array_test)):\n",
        "        dist=[]\n",
        "        sub_vect=[]\n",
        "        for i in range(len(emb_array_train)):\n",
        "            sub_vect.append((emb_array_test[k]-emb_array_train[i]))\n",
        "            # print(len(sub_vect[0][0]))\n",
        "            # print(sub_vect[0][0][0])\n",
        "            # print(len(sub_vect[0]))\n",
        "\n",
        "        #print(sub_vect.shape)\n",
        "        for i in range(len(sub_vect)):\n",
        "            s=0\n",
        "            for j in range(len(sub_vect[i])):\n",
        "                # print(sub_vect[i][0][])\n",
        "                s=s+sub_vect[i][j]*sub_vect[i][j]\n",
        "            s=np.sqrt(s)\n",
        "            dist.append(s)\n",
        "        print(dist)\n",
        "        min=dist[0]   \n",
        "        j=0 \n",
        "        for i in range(len(dist)):\n",
        "            if dist[i]<min:\n",
        "                min=dist[i]\n",
        "                j=i\n",
        "        if min<1:\n",
        "            print(\"anh duoc nhan dang la cua   {}\".format(class_names[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQsWOWnK1cKA",
        "colab_type": "text"
      },
      "source": [
        "Test program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEKEaae415Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_train=\"put your path here\"\n",
        "out_put_mtcnn=\"put your path here\"\n",
        "model_path=\"put your path here\"\n",
        "output_data_dir=\"put your path here\"\n",
        "output_data_path=\"put your path here\"\n",
        "\n",
        "path_test=\"put your path here\"\n",
        "out_put_mtcnn_test=\"put your path here\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fe-Kam71b70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#process for train_Data\n",
        "dataset_train=get_dataset(path_train)\n",
        "align_mtcnn(dataset_train,out_put_mtcnn)\n",
        "emb_array_train,labels=main(input_path=out_put_mtcnn,dataset=dataset_train,model_path=model_path)\n",
        "save_distance(dataset= dataset_train,output_path=output_data_path,emb_train= emb_array_train,name_output_directory=output_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jH5oyKJ1oUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#process for train_Data\n",
        "output_path=output_data_path\n",
        "dataset_test=get_dataset(path_test)\n",
        "print(dataset_test[0].image_paths)\n",
        "align_mtcnn(dataset_test,out_put_mtcnn_test)\n",
        "emb_array_test,labels=main(input_path=out_put_mtcnn_test,dataset=dataset_test,model_path=model_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz5QG2cO4CLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#recognize\n",
        "nrof_images=2\n",
        "nrof_flips=3;\n",
        "image_paths=4\n",
        "labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n",
        "print(labels_array)\n",
        "print(np.array(image_paths))\n",
        "print(np.repeat(np.array(image_paths),nrof_flips))\n",
        "image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n",
        "print(\"image_paths_array\",image_paths_array)\n",
        "\n",
        "control_array = np.zeros_like(labels_array, np.int32)\n",
        "print(\"control_array\",control_array)\n",
        "print(4/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHuJi3CM4KBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"label name of test  {}\".format(labels))\n",
        "calculate_recognize(emb_array_test,output_data_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}